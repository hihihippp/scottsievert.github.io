<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Math | Scott Sievert]]></title>
  <link href="http://scottsievert.github.io/blog/categories/math/atom.xml" rel="self"/>
  <link href="http://scottsievert.github.io/"/>
  <updated>2014-11-02T08:47:43-06:00</updated>
  <id>http://scottsievert.github.io/</id>
  <author>
    <name><![CDATA[Scott Sievert]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Mathematics Behind XKCD #688]]></title>
    <link href="http://scottsievert.github.io/blog/2014/10/31/mathematics-behind-xkcd-number-688/"/>
    <updated>2014-10-31T09:56:00-05:00</updated>
    <id>http://scottsievert.github.io/blog/2014/10/31/mathematics-behind-xkcd-number-688</id>
    <content type="html"><![CDATA[<p>What are the mathematics behind <a href="http://xkcd.com/688/">XKCD #688</a>? I <a href="https://github.com/scottsievert/xkcd-688">implemented</a> it myself but didn’t grasp the theory behind it but that’s been covered by a course<sup id="fnref:course"><a href="#fn:course" rel="footnote">1</a></sup> I’m taking. While I want to touch on the applications, I won’t attempt to teach something that took me two months to learn.</p>

<!--More-->

<p><img src="http://imgs.xkcd.com/comics/self_description.png" alt="xkcd comic" /></p>

<p>I’m betting Randall implemented this with a for-loop even though he probably knows the theory. It’s what I would also do – a for-loop is just too easy.</p>

<p><code>python
x = define_source_image()
for i in arange(7):
    x = process_image(x)
</code></p>

<p><img src="https://github.com/scottsievert/xkcd-688/raw/master/out.gif" alt="animated comic" /></p>

<p>This is the natural method, but the theory has some interesting applications. While we could see the theory with the image, we can see it more clearly with the <a href="http://en.wikipedia.org/wiki/Fibonacci_number">Fibonacci numbers</a>. The Fibonacci numbers just add up the two previous Fibonacci numbers. These are typically implemented<sup id="fnref:code"><a href="#fn:code" rel="footnote">2</a></sup> in the naïve code below. As we’re just calling the equivalent of some function in a for loop, this is very similar to the XKCD code.</p>

<p><code>python
def fibonacci(k):
    """ calculate the k-th fibonacci number """
    x = array([0, 1, nan]) # first two fibonacci numbers
    for i in arange(k):
        x[2] = x[0] + x[1]
        x[0:2] = x[1:]
    return x[2]
</code></p>

<p>Mathematically, it can be represented as $x_k = x_{k-1} + x_{k-2}$ with $x_0 = 0$ and $x_1 = 1$. While this is valid, it doesn’t reveal the entire theory. Instead, let’s choose to represent it with a matrix:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

    \begin{bmatrix}
    x_k \\
    x_{k-1} \\
    \end{bmatrix}
    =
    \mathbf{x_k}
    =
    \mathbf{A} \cdot \mathbf{x}_{k-1}
    =
    \begin{bmatrix}
    1 & 1 \\
    1 & 0 \\
    \end{bmatrix}
    \begin{bmatrix}
    x_{k-1} \\
    x_{k-2} \\
    \end{bmatrix}
 %]]&gt;</script>

<p>Calculating the Fibonacci numbers is also an example of <a href="https://en.wikipedia.org/wiki/Fixed_point_(mathematics)">fixed point iteration</a>. With this method if we wanted to calculate the $k$th Fibonacci number $x_k$ we’d have do $k$ matrix multiplications. </p>

<script type="math/tex; mode=display">% &lt;![CDATA[

    \begin{align*}
    \mathbf{x}_k &= \mathbf{A}\cdot \mathbf{x}_{k-1} \\ 
    &= \mathbf{A}\cdot \left( \mathbf{A}\cdot \mathbf{x}_{k-2} \right) \\
    &= \mathbf{A}\cdot \mathbf{A}\cdot \mathbf{A} \cdots \mathbf{A} \cdot \mathbf{x}_0 \\
    &= \mathbf{A}^k \cdot \mathbf{x}_0
    \end{align*}
 %]]&gt;</script>

<p>We’d just compute $\mathbf{A}^k$ by using <a href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.matrix_power.html"><code>np.linalg.matrix_power</code></a> to implement this, but naïve <a href="http://en.wikipedia.org/wiki/Matrix_multiplication">matrix multiplication</a> is expensive timewise. Calculating $\mathbf{x}_k$ naïvely has a <a href="https://en.wikipedia.org/wiki/Computational_complexity_theory">computational complexity</a> of $O(k\cdot n^3)$ when $\mathbf{A}$ is a $n \times n$ matrix. The cost of this algorithm drastically increases with $n$. This could be the number of pixels in our image or the number of locations a GPS records. Powerful algorithms like the FFT with their complexity of $O(n \log n)$ drastically outweigh this seemingly simple Fibonacci calculation.</p>

<p>For the Fibonacci case, $\mathbf{A}$ remains constant and math has several nice ways<sup id="fnref:other"><a href="#fn:other" rel="footnote">3</a></sup> to speed things up. One convenient way is to find a diagonal representation of the matrix called $\mathbb{\Lambda}$ because then we could find $\mathbf{A}^k$ easily. To do that, we’ll need to find $\mathbb{\mathbb{\Lambda}}^k$ first. </p>

<script type="math/tex; mode=display">% &lt;![CDATA[

    \mathbb{\Lambda}^k = 
    \mathbb{\Lambda} \cdot \mathbb{\Lambda} \cdot \ldots \cdot \mathbb{\Lambda} =
     \begin{bmatrix}
       \lambda_{_1}^k    &                &        &                \\
                         & \lambda_{_2}^k &        &                \\
                         &                & \ddots &                \\
                         &                &        & \lambda_{_n}^k \\
     \end{bmatrix}
 %]]&gt;</script>

<p>But we can’t just arbitrarily lose information. It turns out that for the right choices<sup id="fnref:calculate"><a href="#fn:calculate" rel="footnote">4</a></sup> of $\mathbf{Q}$ and $\mathbb{\Lambda}$ involving eigenvalues and eigenvectors we get<sup id="fnref:limit"><a href="#fn:limit" rel="footnote">5</a></sup> $\mathbf{A} = \mathbf{Q} \cdot\mathbb{\Lambda}\cdot \mathbf{Q}^{-1}$. We really want $\mathbf{A}^k$ for our fixed point iteration, and we can get it in terms of $\mathbb{\Lambda}$ and $\mathbf{Q}$ by using the fact that $\mathbf{Q}\cdot\mathbf{Q}^{-1} = \mathbf{I}$ and anything times this identity $\mathbf{I}$ is itself:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

    \begin{align*}
    \mathbf{A}^k &= (\mathbf{Q}\cdot \mathbb{\Lambda}\cdot \mathbf{Q}^{-1}) \cdot (\mathbf{Q}\cdot \mathbb{\Lambda}\cdot \mathbf{Q}^{-1}) \cdots (\mathbf{Q}\cdot \mathbb{\Lambda}\cdot \mathbf{Q}^{-1}) \\
    &= \mathbf{Q} \cdot \mathbb{\Lambda}^k\cdot \mathbf{Q}^{-1}
    \end{align*}
 %]]&gt;</script>

<p>Because of this added $\mathbf{Q}$ we find that computing $\mathbf{A}^k$ takes $O(n\log n)$ time assuming we’re given  $\mathbb{\Lambda}$. Computing $\mathbb{\Lambda}$ requires $O(n^3)$ time meaning that $\mathbf{A}^k$ can be computed in $O(n\log n + n^3) \approx O(n^3)$ time. Over the naïve code for the simple case, this is a large speed up! A factor of $k$ – which could be as large as a hundred – is nothing to sneer at!</p>

<p>But… wait. We don’t care about every value in the vector this Fibonacci case. We only care about $x_k$. We already have a <a href="https://en.wikipedia.org/wiki/Closed-form_expression">closed form solution</a> for $\mathbf{x}_k$ but after doing the matrix multiplication we find that $x_k$ has a simple formula. With the associated matrices $\mathbf{Q}$ and $\mathbb{\Lambda}$ we find that</p>

<script type="math/tex; mode=display">
x_k = \frac{1}{\sqrt{5}} (\lambda_1^k - \lambda_2^k) = 
\frac{1}{\sqrt{5}} \left[
    \left(\frac{1+\sqrt{5}}{2}\right)^k - \left(\frac{1-\sqrt{5}}{2}\right)^k
\right]
</script>

<p>Let’s think about what I’m saying here. I’m saying that a formula involving three irrational numbers with two raised to a power is an integer <em>and</em> the $k$th Fibonacci number. I mean, even the irrational <a href="https://en.wikipedia.org/wiki/Golden_ratio">golden ratio</a> $\phi = (1 + \sqrt{5})/2 = 1.618\cdots$ appears! The last thing we would expect is a rational number, much less an integer. But the theory is solid and the first values of $x_k$ are $0, 1, 1, 2, 3, 5, 8$ – the Fibonacci numbers!</p>

<p>This means that we can just define our <code>fibonacci</code> function to run <em>fast</em>. If we want a single value, this algorithm runs in $O(1)$ time vs the naïve code that ran in $O(k)$ time. </p>

<p><code>python
def fibonacci(k):
    """ returns the k-th fibonacci number """
    l_1 = (1 + sqrt(5)) / 2
    l_2 = (1 - sqrt(5)) / 2
    return (l_1**k - l_2**k) / sqrt(5)
</code></p>

<p>This theory of eigenvalues and eigenvectors <em>must</em> have other insights. Eigenvalues applied themselves well to the Fibonacci problem; where else could they be used? </p>

<p>The most obvious application is with <a href="https://en.wikipedia.org/wiki/Linear_stability">stability</a>. The definition of stability I’ll use is that a system (which can be represented by a matrix) is stable if it doesn’t blow up towards infinity. Since $\left|x_k\right| \propto \left(\left|\lambda\right|_\max\right)^k$ for large $k$, this system only converges if $ \left|\lambda \right|_\max &lt; 1$ and blows up if $\left| \lambda \right|_\max &gt; 1$. We can see that clearly with our Fibonacci sequence as our largest eigenvalue is the golden ration and greater than one. As expected, the Fibonacci numbers blow up towards infinity.</p>

<p>Can we just glance at a matrix to tell if the corresponding system is stable? This involves complex problems like factoring an $n$th degree polynomial and finding a <a href="https://en.wikipedia.org/wiki/Determinant">determinant</a>. For the general case, there’s no simple easy method besides the computer sitting at your hands with the function <code>eig</code> in <code>numpy.linalg</code>. Take that with a grain of salt as there might be some relation between eigenvalues and the <a href="https://en.wikipedia.org/wiki/Z-transform">z-transform</a> because a <a href="https://en.wikipedia.org/wiki/Causality">causal</a> system only converges if it has poles $\left|p\right|$ with $\left|p\right| &lt; 1$. This also involves factoring an $n$th degree polynomial but it’s a little easier to obtain.</p>

<p>Stability is critical for differential equations, and eigenvalues play a critical role in determining stability for certain differential equations. As you may know, differential equations govern our world. Examples of differential equations are found everywhere – they govern the size of animal populations and how airplanes fly.</p>

<p>Another more interesting application is with social networks. If you want to see how many friend groups are in some social network, you can just count the eigenvalues that are 0. But there are people I only interact with occasionally; are we in the same friend group? Accordingly, calculating small eigenvalues is much more time intensive.</p>

<p>There are more uses of eigenvalues including machine learning concepts such as principle component analysis and face recognition. One interpretation of eigenvalues is that they’re a convenient way of representing a matrix with some very nice properties but I’m betting there’s more as it seems that any interesting application involves eigenvalues in some way.</p>

<div class="footnotes">
  <ol>
    <li id="fn:course">
      <p>This course is called Applied Linear Algebra. I covered why this course name sounds basic in a <a href="http://scottsievert.github.io/blog/2014/07/31/common-mathematical-misconceptions/">previous blog post</a>. This covered the seemingly simple names of dimensions, linear functions and linear algebra and eventually built up to why computers are nesessary.<a href="#fnref:course" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:code">
      <p>I don’t want to clutter up the code so I use <code>from pylab import *</code>. Yes it is bad practice… but man it’s nice.<a href="#fnref:code" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:other">
      <p>The current limit for computing $A^k$ is done with the <a href="http://en.wikipedia.org/wiki/Coppersmith–Winograd_algorithm">Coppersmith-Winograd algorithm</a> with a computation complexity of $O(n^{2.3727}\log k)$.<a href="#fnref:other" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:calculate">
      <p>If you want to calculate $Q$ and $\mathbb{\Lambda}$, see the <a href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors">wiki page</a>. More intuition is present in a <a href="http://math.stackexchange.com/questions/36815/a-simple-explanation-of-eigenvectors-and-eigenvalues-with-big-picture-ideas-of">StackExchange question</a>.<a href="#fnref:calculate" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:limit">
      <p>Well, there are certain restrictions, one being $A$ has to have distinct eigenvalues.<a href="#fnref:limit" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Common Mathematical Misconceptions]]></title>
    <link href="http://scottsievert.github.io/blog/2014/07/31/common-mathematical-misconceptions/"/>
    <updated>2014-07-31T21:12:26-05:00</updated>
    <id>http://scottsievert.github.io/blog/2014/07/31/common-mathematical-misconceptions</id>
    <content type="html"><![CDATA[<p>When I heard course names for higher mathematical classes during high school
and even part of college, it seemed as if they were teaching something simple
that I learned back in middle school. I knew that couldn’t be the case, and
three years of college have taught me otherwise.</p>

<!--More-->

<h2 id="n-dimensions">N dimensions</h2>
<p>Generalizing to $N$ <a href="https://en.wikipedia.org/wiki/Dimension_(mathematics_and_physics)">dimensions</a> is often seen as a pointless mathematical
exercise because of a language confusion. Space exists in three dimensions; we
just need three elements or a three dimensional vector to describe any point.
Because of this, we say we live in three dimensions. So isn’t generalizing to
an $N$ dimensions pointless?</p>

<p>Let’s go back to <a href="https://en.wikipedia.org/wiki/Vector_space">vectors</a>. An $N$ dimensional vector is just one with $N$
components.<sup id="fnref:numpy"><a href="#fn:numpy" rel="footnote">1</a></sup> Thinking about a grid or 2D graph, we need two numbers to
describe any point on that grid, $x$ and $y$. That’s a two dimensional vector.
For the four dimensions we live in, we need four: $x, y, z, t$.</p>

<p>But wait. Why can’t we define a five dimensional vector that includes my age or
a six dimensional vector that also includes my year in school? Thinking about
this in the real world, we have data that has $N$ components all the time. Images
on my computer have $N$ pixels and my GPS records $N$ locations. Almost anything
<em>discrete</em> is an $N$ dimensional vector.</p>

<p>This pops up all the time in image processing, machine learning and almost
anywhere that uses linear algebra <em>or a computer.</em> All the vectors and matrices
I’ve seen have $N$ components, a <em>discrete</em> number. Computers only have a
finite number of bits<sup id="fnref:computer"><a href="#fn:computer" rel="footnote">2</a></sup> meaning computers must also be discrete. That
means if we want to do anything fancy with a computer, we need to use linear
algebra.</p>

<h2 id="linear-algebra">Linear algebra</h2>
<p>At first glance, <a href="https://en.wikipedia.org/wiki/Linear_algebra">linear algebra</a> seems like middle school material. In
middle school you might have seen $y=m\cdot x+b$ (more on this linear function
later). In linear algebra, you might see $\mathbf{y}=\mathbf{A\cdot x+b}$ where $\mathbf{y, x,}$ and $\mathbf{b}$ are
vectors and $\mathbf{A}$ is a matrix.<sup id="fnref:font"><a href="#fn:font" rel="footnote">3</a></sup></p>

<p>A matrix is just nothing more than a collection of vectors and 2D grid of
numbers. For example, if you have $M$ students with $N$ features (age, weight,
GPA, etc), you can collect them by simply stacking the vectors and make an
$M\times N$ matrix.</p>

<p>We’ve defined both vectors and matrices, but what about basic operations? How
do we define addition and multiplication? Addition works element-wise, but
multiplication is a bit more interesting.</p>

<p><a href="https://en.wikipedia.org/wiki/Matrix_multiplication">Matrix multiplication</a> is defined rather simply, but that doesn’t give you any
intuition. You can think of matrix multiplication as linear combinations of the
rows and columns, but that still doesn’t tell what matrix multiplication
<em>means.</em></p>

<p>Intuitively, we know that we can transform any matrix into any other matrix
because we can arbitrarily choose numbers. But wait. In middle school we
learned that a function could transform any number into any other number.
Matrices are then analogous to functions!</p>

<p>For example, let’s say we have a set of inputs $\mathbf{x} = [1,~2,~3,~4]^T$ and we want to
perform the function $y = f(x) = 2\cdot x + 1$ on each element in our vector. In
matrix notation, we can just do </p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\mathbf{y} = 
\begin{bmatrix} 2&0&0&0 \\ 0&2&0&0 \\ 0&0&2&0 \\ 0&0&0&2 \end{bmatrix} 
\cdot
\begin{bmatrix} 1 \\ 2 \\ 3 \\ 4 \end{bmatrix}
+
\begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix}
 %]]&gt;</script>

<p>With these functions, we can perform even more powerful actions. We can easily
swap elements or perform different actions on different elements. It gets even
more powerful but still relatively simple to an experienced mathematician. We
can find when a matrix or function simply scales a vector by finding the
<a href="https://en.wikipedia.org/wiki/Eigenvalue">eigenvalues and eigenvectors</a> or use the <a href="https://en.wikipedia.org/wiki/Inverse_matrix">inverse</a> to find a discrete
analog to $f^{-1}(x)$.</p>

<h2 id="linear-functions">Linear functions</h2>
<p>A <a href="https://en.wikipedia.org/wiki/Linear_function">linear function</a> seems like the most boring function. It’s just a straight
line, right? It is, but English got confused and it also means when some
function obeys the <a href="https://en.wikipedia.org/wiki/Superposition_principle">superposition principle</a>. Concisely in math, it’s
when $f(\alpha x+\beta y)=\alpha\cdot f(x)+\beta\cdot f(y)$.</p>

<p>This means that <em>a general linear function is not linear.</em> If $f(x) = mx+b$,
$f(x+y) = f(x)+f(y)-b$ which doesn’t satisfy the definition of a linear
function. But there’s much more interesting functions out there.  <a href="https://en.wikipedia.org/wiki/Integral">Integration</a>
and <a href="https://en.wikipedia.org/wiki/Derivative">differentiation</a> respect scalar multiplication and addition, meaning
they’re linear.</p>

<p>Since a host of other functions depend on integration and differentiation, so
many of these  functions (but not all!) are also linear.<sup id="fnref:linear"><a href="#fn:linear" rel="footnote">4</a></sup> The 
<a href="https://en.wikipedia.org/wiki/Discrete_Fourier_transform">discrete Fourier transform</a> (computed by <code>fft</code>), <a href="https://en.wikipedia.org/wiki/Wavelet_transform">wavelet transform</a> and a host of other
functions are linear.</p>

<p>Addition and scalar multiplication are defined element-wise for matrices, so
any linear function can be represented by a matrix. There are matrices for
computing the <code>fft</code> and wavelet transform. While unused as they require
$O(n^2)$ operations, they exist. Exploiting specific properties, mathematicians
are often able to push the number of operations down to $O(n\log(n))$.</p>

<p>Linear functions are perceived as boring. If you know $\mathbf{y}$ in $\mathbf{y=Ax}$ for
some known<sup id="fnref:mat_comp"><a href="#fn:mat_comp" rel="footnote">5</a></sup> matrix $\mathbf{A}$, you can easily find $\mathbf{x}$ by left-multipying
with $\mathbf{A}^{-1}$. This might be expensive time-wise, but an exact solution is
computable.  </p>

<p>Nonlinear functions have a unique property that an exact and closed form
solution <a href="https://en.wikipedia.org/wiki/List_of_nonlinear_partial_differential_equations#Exact_solutions">often can’t be found</a>. This means that no combination of
elementary function like $\sin(\cdot), \exp(\cdot), \sqrt{\cdot},\int \cdot~dx,
\frac{d~\cdot}{dx}$ along with respective operators and the infinitely many
real numbers can describe <em>every</em> solution.<sup id="fnref:solution"><a href="#fn:solution" rel="footnote">6</a></sup> Instead, those elementary
functions can only describe the equation that needs to be solved.</p>

<p>Even a “simple” problem such as 
<a href="https://en.wikipedia.org/wiki/Pendulum_(mathematics)">describing the motion of a pendulum</a> 
is nonlinear and an exact solution can’t be found, even for the most simple
case. Getting even more complex,
there’s a whole list of 
<a href="https://en.wikipedia.org/wiki/List_of_nonlinear_partial_differential_equations">nonlinear parital differential equations</a> 
that solve important problems.<sup id="fnref:head"><a href="#fn:head" rel="footnote">7</a></sup></p>

<p>This is why simulation is such a big deal. No closed form solution can be found
meaning that you have to find a solution numerically.  Supercomputers don’t
exist to for mild speed boosts or storage requirements that machine
learning or “big data”<sup id="fnref:buzz"><a href="#fn:buzz" rel="footnote">8</a></sup> seems to require. No, supercomputers exist
because with a nonlinear problem, a simulation <em>has</em> to be run to get <em>any</em>
result. Running one of these simulations on my personal computer would take
years if not decades.</p>

<p>When you hand this type of problem to an experienced mathematician, they won’t
look for a solution. Instead, they’ll look for bounds and guarantees on a
solution they work out. If they just came up with a solution, they wouldn’t
even know how good it is! So, they’ll try to bound the error and look for ways
to lower that error.</p>

<p>When I started college, I was under the impression that my later career would
involve long and messy exact solutions. In my second and third years, I came to
realize that those messy solutions didn’t exist and instead we used extremely
elegant math to find exact solutions. In the last couple months<sup id="fnref:nonlinear"><a href="#fn:nonlinear" rel="footnote">9</a></sup>, I have come to
realize that while simple math might <em>describe</em> the problem, there’s no closed
form solution and the only way to get a solution to “interesting” problems is
with a computer.</p>

<h2 id="section"></h2>

<div class="footnotes">
  <ol>
    <li id="fn:numpy">
      <p>And hence an ndarray or “N dimensional array” in NumPy<a href="#fnref:numpy" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:computer">
      <p>When computers can have an <a href="(https://en.wikipedia.org/wiki/Uncountable_set)">uncountably infinite</a> number of bits they can be continuous and I’ll eat my hat<a href="#fnref:computer" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:font">
      <p>For the rest of this post, I’ll use that bold mathematical notation to describe vector and the regular font to describe scalars<a href="#fnref:font" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:linear">
      <p>That is if they only depend on the simplest forms of integration and differentiation<a href="#fnref:linear" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:mat_comp">
      <p>And matrix completion handles when $A$ isn’t known. This is analgous to finding an unknown scalar function<a href="#fnref:mat_comp" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:solution">
      <p>It should be noted that <em>occaisonally</em> a closed form solution can be found <em>assuming</em> certain conditions apply<a href="#fnref:solution" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:head">
      <p>And these problems are wayyy over my head<a href="#fnref:head" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:buzz">
      <p>Yeah it’s a buzz word. For example, satellites that collect 2 billion measurements a day. That’s a lot of data, but analyzing it and using it is not “big data.”<a href="#fnref:buzz" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:nonlinear">
      <p>Or even as I write this blog post. I’ve known this for a while but this is a new light<a href="#fnref:nonlinear" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fourier Transforms and Optical Lenses]]></title>
    <link href="http://scottsievert.github.io/blog/2014/05/27/fourier-transforms-and-optical-lenses/"/>
    <updated>2014-05-27T13:50:25-05:00</updated>
    <id>http://scottsievert.github.io/blog/2014/05/27/fourier-transforms-and-optical-lenses</id>
    <content type="html"><![CDATA[<p>The <a href="https://en.wikipedia.org/wiki/Fourier_transform">Fourier transform</a> and it’s <a href="http://bsp.pdx.edu/Reports/BSP-TR0201.pdf">closely related</a>
cousin the discrete time Fourier transform (computed by the FFT) is a powerful
mathematical concept. It breaks an input signal down into it’s frequency
components. The best example is lifted from Wikipedia.</p>

<!--More-->

<p><img class="right" src="https://upload.wikimedia.org/wikipedia/commons/7/72/Fourier_transform_time_and_frequency_domains_%28small%29.gif" width="200"></p>

<p>The Fourier transform is used in almost every type of analysis. I’ve seen the
discrete Fourier Transform used to detect vehicles smuggling contraband
across borders and to seperate harmonic overtones from a cello. It can
transform <a href="https://en.wikipedia.org/wiki/Convolution">convolution</a> into a simple (and fast!) multiplication and
multiply incredibly long polynomials.  These might seem pointless, but they’re
useful with any “<a href="https://en.wikipedia.org/wiki/LTI_system_theory">nice</a>” system and complicated stability problems,
respectively. The Fourier transform is perhaps the most useful abstract
athematical concept.</p>

<p>We can see that it’s very abstract and mathematical just by looking at it’s
definition: $\newcommand{\fourier}[1]{\mathbb{F}\left[ #1 \right]} $</p>

<script type="math/tex; mode=display"> F(f_x) = \fourier{f(x)} = \int f(x) \exp\left[ -j \cdot 2\pi \cdot f_x \cdot x \right] dx</script>

<p>The Fourier transform is so useful, the discrete version is implemented in
probably every programming language through <code>fft</code> and there are even dedicated
chips to perform this efficiently. The last thing we would expect is for this
abstract and mathematical concept to be implemented by physical devices.</p>

<p>We could easily have some integrated chip call <code>fft</code>, but that’s not
interesting. If a physical device that has some completely unrelated purpose
but can still perform an Fourier transform without human intervention (read:
programming), that’d be interesting. For example, an optical lens is shaped
solely to produce an image, not to take a Fourier transform… but that’s
exactly what it does.</p>

<p><img class="right" src="https://raw.githubusercontent.com/scottsievert/scottsievert.github.io/master/src/source/_posts/lens_fft_images/setup.png" width="300"></p>

<p>Let me repeat: a <em>lens can take an exact spatial Fourier transform.</em> This does
have some limitations<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>, mainly that it only works under coherent light. A
coherent light source is simply defined a source that’s not random. Natural
light is random as there are many different wavelengths coming in at random
times. Laser light is very coherent – there’s a very precise wavelength and
every individual ray is in phase<sup id="fnref:3"><a href="#fn:3" rel="footnote">2</a></sup>.</p>

<p><a href="http://www.amazon.com/Introduction-Fourier-Optics-Joseph-Goodman/dp/0974707724">Goodman</a>, the textbook that almost every Fourier optics course uses, says<sup id="fnref:2"><a href="#fn:2" rel="footnote">3</a></sup>
that the field for a lens of focal length $f$ is</p>

<script type="math/tex; mode=display"> 
U_f(u,v) = 
\frac{
    A \exp\left[ j \frac{k}{2f} (1 - \frac{d}{f}) (u^2 + v^2) \right]
                }{j \lambda f}
    \cdot\\
    \int \int U_o(x,y) \exp\left[ -j \frac{2\pi}{\lambda f} (xu + yv) \right]
    dxdy
</script>

<p>When $d=f$, <em>that’s exactly the definition of a Fourier transform.</em> Meaning we
can expect $U_f(u,v) \propto \fourier{U_i(x,y)}\big|_{f_x = u/\lambda f} $. Minus
some physical scaling, that’s an <em>exact</em> Fourier transform.</p>

<p>You may know that a purely real values input to a Fourier transform gives a
complex output. We have no way of detecting this complexity or phase. Instead,
our eyes (and cameras/etc) detect <em>intensity</em> or the <em>magnitude</em> of
$U_f(x,y)$. That is, they detect $I_f(x,y) = \left|U_f(x,y)\right| ^2$. This function is
purely real, although there’s some <a href="http://www.opticsinfobase.org/ao/fulltext.cfm?uri=ao-23-6-812&amp;id=27347">fancy ways</a><sup id="fnref:6"><a href="#fn:6" rel="footnote">4</a></sup> to detect phase as well.</p>

<p>No matter how elegant this math was, I wanted to see it in the real world and
compare the computer FFT with the lens Fourier transform.  The setup for this
experiment was rather involved, and I would like to give a resounding thanks to
Mint Kunkel.  Without his help, I <em>never</em> could have gotten an image, much less
a decent one.</p>

<p>I was taking an image of a grid, illuminated by a infinite<sup id="fnref:4"><a href="#fn:4" rel="footnote">5</a></sup> plane wave
generated by a laser. The computer generated image and discrete Fourier
Transform are shown below.</p>

<p><img class="center" src="https://raw.githubusercontent.com/scottsievert/scottsievert.github.io/master/src/source/_posts/lens_fft_images/lens-fft-computer/grid.png" width="600"></p>

<p>We can’t expect the lens Fourier transform to look exactly like this. The first
issue that jumps to mind is discrete vs continuous time, but that should
hopefully play a small role. The equipment required to get this image is highly
specialized and costs more than I want to know. But even more significantly,
the tuning of this equipment is critical and almost impossible to get right.
Plus, there’s detail like grid spacing/size/etc missing, the reason the two
images aren’t almost exactly identical.</p>

<p>Regardless, the Fourier transform by lens shows remarkable similarity to the
Fourier transform done on the computer.</p>

<p><img class="center" src="https://raw.githubusercontent.com/scottsievert/scottsievert.github.io/master/src/source/<em>posts/lens_fft_images/bench/grid</em>0001.png" width="500"></p>

<p>This is a real world example of how a lens, a simple object used for
photography performs perhaps the most powerful concept in signal processing.
This alone is cool, but it shows itself elsewhere. The transfer function is
just the pupil function or <script type="math/tex">H\left(f_x, f_y\right) = P(cx,cy) </script> ($c$ is a
constant, only works under coherent light, but has similar effect in incoherent
light). If you want to resolve a higher frequency (aka more detail), you need
your pupil function to extend further.</p>

<p><img class="right" src="https://raw.githubusercontent.com/scottsievert/scottsievert.github.io/master/src/source/_posts/lens_fft_images/pupil/stack.png" width="300"></p>

<p>Animals have different shaped <em>pupils</em> or different <em>pupil functions</em> for their
eye. A cat has a very vertical pupil, a zebra’s pupil is horizontal and an
eagle’s pupil is round<sup id="fnref:5"><a href="#fn:5" rel="footnote">6</a></sup>. There are different evolutionary reasons why an animal
needs to see more detail in the vertical or horizontal directions (ie, jumping
vs hunting) and this shows itself with their pupils. Animals see more detail in
the horizontal or vertical directions if that’s what they care about!</p>

<!--XXX: check!-->

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Another limitation: the lens can only accept frequencies as large as $r/\lambda f$, meaning the input signal must be band-limited.<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>For a detailed explanation of why the laser spots are speckled and random, see my <a href="http://scottsievert.github.io/blog/2014/05/18/speckle-and-lasers/">previous blog post</a>.<a href="#fnref:3" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>The derivation culminates on page 105. This assumes the <a href="http://en.wikipedia.org/wiki/Paraxial_approximation">paraxial approximation</a> is valid.<a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:6">
      <p>Horner, J. L., &amp; Gianino, P. D. (1984). <a href="http://www.opticsinfobase.org/ao/fulltext.cfm?uri=ao-23-6-812&amp;id=27347">Phase-only matched filtering. Applied optics</a>, 23(6), 812-816.<a href="#fnref:6" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>Three centimeters is about infinity according to Mint.<a href="#fnref:4" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p>Images are taken from Google Images. If you are the owner of one of these images and would like it removed, let me know and I’ll remove it.<a href="#fnref:5" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Speckle and Lasers]]></title>
    <link href="http://scottsievert.github.io/blog/2014/05/18/speckle-and-lasers/"/>
    <updated>2014-05-18T09:26:40-05:00</updated>
    <id>http://scottsievert.github.io/blog/2014/05/18/speckle-and-lasers</id>
    <content type="html"><![CDATA[<p>We know that lasers are very accurate instruments and emit a very precise
wavelength and hence are in an array of precision applications including
<a href="https://en.wikipedia.org/wiki/Bloodless_surgery">bloodless surgery</a>, <a href="https://en.wikipedia.org/wiki/Laser_eye_surgery_(disambiguation)">eye surgery</a> and 
<a href="https://en.wikipedia.org/wiki/Fingerprint">fingerprint detection</a>. 
So why do we see random light/dark spots
when we shine a laser on anything? Shouldn’t it all be the same color since
lasers are deterministic (read: not random)?  To answer that question<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>, we
need to delve into optical theory.</p>

<!--More-->

<p><a href="https://en.wikipedia.org/wiki/Coherence_(physics)"><em>Coherent</em> optical systems</a> are simply defined to be
<em>deterministic</em> systems. That’s a big definition, so let’s break it into
pieces. Coherent systems are where you know the wavelength and phase of every
ray. Lasers are very coherent (one wavelength, same phase) while sunlight is
not coherent (many wavelength, different phases).</p>

<p>Deterministic is just a way of saying everything about the system is known and
there’s no randomness. Sunlight is not deterministic because there are many
random processes. Photons are randomly generated and there are many
wavelengths. Sunspots are one example of the randomness present in sunlight.</p>

<p>But if lasers are coherent and deterministic, why do we see speckle (read:
bright and dark spots) when we see a laser spot? The speckle is random; we
can’t predict where every dark spot will be. The randomness of this laser spot
and the fact that lasers are deterministic throws a helluva question at us. It
turns out <em>what</em> we see the laser on is important, but let’s look at the math
and physics behind it.</p>

<p>Coherent optical systems have a very special property. Their 
<a href="https://en.wikipedia.org/wiki/Impulse_response">impulse response</a> (read: reaction to a standardized input)
in the frequency domain is just the pupil function.  For those familiar with
the parlance and having $f_x$ be a spatial frequency as opposed to a time
frequency,</p>

<script type="math/tex; mode=display">H\left( f_x, f_y\right) = P(-\lambda z x, -\lambda z y) </script>

<p>When I saw this derived<sup id="fnref:3"><a href="#fn:3" rel="footnote">2</a></sup>, I thought “holy (expletive).” If you just want to only pass high
frequency spatial content (read: edges), then all that’s required it to not let
light through the center of the lens.</p>

<p><img class="right" src="https://raw.githubusercontent.com/scottsievert/side-projects/master/speckle/impulse_respone.png" width="200"></p>

<p>Since this system is linear, we can think of our output as bunch of impulse
responses shifted in space and scaled by the corresponding amount. This is the
definition of <a href="https://en.wikipedia.org/wiki/Convolution">convolution</a> and only works because this is a 
<a href="https://en.wikipedia.org/wiki/LTI_system_theory">linear and space invariant system.</a></p>

<p>To find our impulse response in the space domain, $h\left( x, y\right) $, we
have to take the Fourier transform (aka FFT) of our pupil. Since our pupil
function is symmetric, the inverse Fourier transform and forward Fourier
transform <a href="https://en.wikipedia.org/wiki/Fourier_transform#Invertibility_and_periodicity">are equivalent</a>.</p>

<p>```python
# a circular pupil
pupil = zeros((N,N))
i = argwhere(x<strong>2 + y</strong>2 &lt; r**2)
pupil[i[:,0], i[:,1]] = 1</p>

<p>h = fft2(pupil) # our impulse response since H(fx) = P(x)
h = fftshift(h)
```</p>

<!--plane wave spectrum-->
<p>Through the angular plane wave spectrum, this impulse response can be viewed as
a series plane waves coming in at different angles, shown in the figure below.</p>

<p><img class="right" src="https://raw.githubusercontent.com/scottsievert/side-projects/master/speckle/apws.png" width="200"></p>

<p>What angles can a wave be thought of as? The frequency content and angles turn
out to be related, since two planes waves of a constant frequency adding
together can have a change in frequency depending on what angle they’re at,
which makes intuitive sense. Or, our spatial plane wave $U(x,y)$ can be
represented by the Fourier transform:</p>

<script type="math/tex; mode=display">\textrm{APWS}(\theta_x, \theta_y) = \mathcal{F}\left\{ U(x,y) \right\}\rvert_{f_x = \theta_x/\lambda}</script>

<p>The wall which the laser is shining on is not smooth and perfectly flat. It’s
rough, and the distance adds a phase difference between two waves<sup id="fnref:2"><a href="#fn:2" rel="footnote">3</a></sup>. Through the
<a href="https://en.wikipedia.org/wiki/Random_walk">Drunkard’s Walk</a> and the angular plane wave spectrum, if we could
obtain every angle, the laser spot wouldn’t have any speckle. Our eyes are finite in
size, so we can’t obtain every angle or frequency.</p>

<p>Because the wall gives the wave some random phase, we can represent the spot we
see by a 2D convolution with a random phase and the impulse response. This
convolution is just saying that every spot gives the same response multiplied
by some random phase, added together for every point.</p>

<p>```python
x = exp(1j<em>2</em>pi*rand(N,N)) # a bunch of random phases
x *= p # only within the pupil</p>

<p>d = N/10 # delta since our eyes aren’t infinitely big
y = convolve2d(x, h[N/2-d:N/2+d, N/2-d:N/2+d]) # an approximation with d
```</p>

<p>The laser spot <code>y</code> shows some speckle! The speckle varies with how large we
make <code>d</code> (really <code>delta</code> but that’s long); if we include more frequencies and
more of the impulse response, the dots get smaller. To see this, if you hold a
pinhole up to your eye, the speckles will appear larger.</p>

<p><img class="center" src="https://raw.githubusercontent.com/scottsievert/side-projects/master/speckle/speckle.png" width="500"></p>

<p>An intuitive way to think about this involves the impulse response. The impulse
response changes on with the distance and so does the phase. Certain areas add
up to 0 while others add up to 1. There’s a whole probability density function
that goes with that, but that’s goes further into optical and statistical
theory.</p>

<p><strong>tl;dr:</strong> the roughness of the walls add uncertainty in phase and hence speckle</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>the <a href="https://github.com/scottsievert/side-projects/tree/master/speckle">full code</a> is available on Github.<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>This does rely on assumptions by Goodman. Refer to Eqn. 6-20 for more detail (and thanks Mint!)<a href="#fnref:3" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>reddit commenter <a href="http://www.reddit.com/r/Optics/comments/25zyxa/why_are_laser_spots_speckled/chmg1p2">delmar15</a> pointed out that there’s also phase due to the glass it’s shining through (and many other effects). <a href="http://www.amazon.com/Statistical-Optics-Joseph-W-Goodman/dp/0471399167">Statisitcal optics</a> covers that in much more detail.<a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Predicting the Weather]]></title>
    <link href="http://scottsievert.github.io/blog/2013/11/14/predicting-the-weather/"/>
    <updated>2013-11-14T09:44:33-06:00</updated>
    <id>http://scottsievert.github.io/blog/2013/11/14/predicting-the-weather</id>
    <content type="html"><![CDATA[<p>Let’s say that we’re accurately measuring the temperature in both Madison and Minneapolis, but then our temperature sensor in Minneapolis breaks. We could easily install a new sensor, but we would prefer to estimate the temperature in Minneapolis based on the temperature in Madison. </p>

<!--More-->

<p>First, let’s see the temperature difference between the two cities:</p>

<p>$
\newcommand{\ex}[1]{\mathbb{E}\left[ #1 \right]}
$</p>

<!--![Temperature difference](https://raw.githubusercontent.com/scottsievert/side-projects/master/predicting_weather/temp_diff.png)-->

<p><img class="center" src="https://raw.githubusercontent.com/scottsievert/side-projects/master/predicting_weather/temp_diff.png" width="500"></p>

<p>Let’s say we’re collecting the data accurately and are free from the effects of noise. So, let’s gather the data. In this, we’re estimating $X$ from $Y$. The mean temperature difference, or in math terms, $\ex{\left|X-Y\right|} = 4.26^\circ$ ($\ex{\cdot}$ is an operator that finds the mean).</p>

<p>We’re going to a linear estimation process. This process only takes in
information data about the current data and nothing about the general trend of
the seasons. This process just says that the temperature in Minneapolis is 80%
of the temp in Madison plus some constant; fairly simple. Regardless, it’s still
pretty good as Madison and Minneapolis are fairly similar for temperature. The
only thing this estimation requires is some past weather data in Minneapolis to
predict the mean $\ex{X}$ and variance $\propto \ex{X^2}$ nothing more.</p>

<p>We want to minimize the <em>energy</em> of the error, using the $l_2$ norm. This
choice may seem arbitrary, and it kind of is. If this data were sparse (aka
lots of zeros), we might want to use the $l_1$ or $l_0$ norms. But if we’re
trying to minimize cost spent, the $l_0$ or $l_1$ norms don’t do as good of a
job minimizing the amount of dollars spent.</p>

<p>But doing the math,</p>

<script type="math/tex; mode=display">
\min \ex{\left(X-(\alpha Y+\beta)\right)^2} = \\\\\min \ex{X^2} +
\alpha^2\ex{X^2} + \beta^2 + 2\alpha\beta\ex{X} - 2\alpha\ex{XY} - 2\beta\ex{X}
</script>

<p>Since this function is concave (or U-shaped) and $\ex{\cdot}$ a linear function, we can minimize it using derivates on each term.</p>

<script type="math/tex; mode=display">\frac{d}{d\alpha} = 0 = -2 \ex{XY} + 2 \alpha\ex{Y^2} + 2\beta \ex{y}</script>

<script type="math/tex; mode=display">\frac{d}{d\beta} = 0 = -2 \ex{X} + 2\beta + 2\alpha\ex{X}</script>

<p>This linear system of equations is described by $Ax = b$ or </p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{bmatrix} \ex{Y^2} & \ex{Y} \\\\ \ex{X} & 1 \end{bmatrix}
\cdot
\begin{bmatrix} \alpha~/~2 \\\\ \beta~/~2  \end{bmatrix}
=
\begin{bmatrix} \ex{XY}\\\\ \ex{X}\end{bmatrix}
 %]]&gt;</script>

<p>Solving this linear system of equations by multiplying $A^{-1}$ gives us</p>

<script type="math/tex; mode=display">\alpha = 0.929\\\\\beta = 3.14 </script>

<p>On average, the temperature in Minneapolis 92.9% of Madison’s, plus 3 degrees.
Let’s see how good our results are using this $\alpha$ and $\beta$. The
temperature difference between the two cities, but predicting one off the other
is shown below:</p>

<!--![After prediction](https://raw.githubusercontent.com/scottsievert/side-projects/master/predicting_weather/pred_diff.png)-->
<p><img class="center" src="https://raw.githubusercontent.com/scottsievert/side-projects/master/predicting_weather/pred_diff.png" width="500"></p>

<p>That’s <em>exactly</em> what we want! It’s fairly close to the first graph. While
there are areas it’s off, it’s pretty dang close. In fact, on average it’s
within $4.36^\circ$ – fairly close to the original on average temperature
difference of $4.26^\circ$!</p>

]]></content>
  </entry>
  
</feed>
